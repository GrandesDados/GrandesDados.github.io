<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Junit on Grandes Dados</title>
    <link>http://grandesdados.com/tags/junit/</link>
    <description>Recent content in Junit on Grandes Dados</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>pt-br</language>
    <copyright>Todos os direitos reservados - 2015</copyright>
    <lastBuildDate>Sat, 26 Mar 2016 22:33:20 -0300</lastBuildDate>
    <atom:link href="http://grandesdados.com/tags/junit/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Testes de Spark Streaming com JUnit</title>
      <link>http://grandesdados.com/post/testes-spark-streaming-junit/</link>
      <pubDate>Sat, 26 Mar 2016 22:33:20 -0300</pubDate>
      
      <guid>http://grandesdados.com/post/testes-spark-streaming-junit/</guid>
      <description>

&lt;p&gt;Dando continuidade ao artigo sobre &lt;a href=&#34;http://grandesdados.com/post/testes-spark-junit/&#34;&gt;testes no Spark&lt;/a&gt;, apresento agora como fazer testes em jobs streaming. O maior desafio ao criar estes testes é fazer com que eles não dependam do tempo real para executar, pois alguns jobs podem aguardar muitos minutos para gerar um processamento.&lt;/p&gt;

&lt;p&gt;Os jobs Spark do tipo streaming obtém dados de alguma fonte com fluxo contínuo de dados, como um barramento mensagens como o Kafka, logs etc. Os jobs são configurados com um intervalo de tempo e processam todas as mensagens que chegaram dentro deste período.&lt;/p&gt;

&lt;p&gt;A estratégia que adotaremos será criar um relógio virtual, onde poderemos avançar mais rapidamente e simular o relógio real. Desta forma, nossos testes não precisarão esperar o tempo total do streaming para terminarem. Também criaremos uma fila para simular a sequência dos dados. Veja as classes que precisam ser criadas para que tudo isso funcione.&lt;/p&gt;

&lt;h3 id=&#34;relógio-virtual:616fff47681e64d3e324d29372e66883&#34;&gt;Relógio virtual&lt;/h3&gt;

&lt;p&gt;A primeira classe a ser criada define um relógio virtual e sobrescreve a classe original do Spark:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;org.apache.spark.ClockWrapper&lt;/strong&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34; style=&#34;font-size: medium; line-height: 1.2;&#34;&gt;package org.apache.spark

import org.apache.spark.streaming.{StreamingContext, StreamingContextWrapper}

class ClockWrapper(ssc: StreamingContext) {

  private val manualClock = new StreamingContextWrapper(ssc).manualClock
  def getTimeMillis: Long = manualClock.getTimeMillis()
  def setTime(timeToSet: Long) = manualClock.setTime(timeToSet)
  def advance(timeToAdd: Long) = manualClock.advance(timeToAdd)
  def waitTillTime(targetTime: Long): Long = manualClock.waitTillTime(targetTime)

}&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;

&lt;p&gt;A segunda classe também sobrescreve o comportamento natural do Spark para usar um relógio manual:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;org.apache.spark.streaming.StreamingContextWrapper&lt;/strong&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34; style=&#34;font-size: medium; line-height: 1.2;&#34;&gt;package org.apache.spark.streaming

import org.apache.spark.util.ManualClock

class StreamingContextWrapper(ssc: StreamingContext) {
  val manualClock = ssc.scheduler.clock.asInstanceOf[ManualClock]
}&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;

&lt;p&gt;Não se esqueça de criar os pacotes corretos!&lt;/p&gt;

&lt;h3 id=&#34;test-dstream:616fff47681e64d3e324d29372e66883&#34;&gt;Test DStream&lt;/h3&gt;

&lt;p&gt;A próxima classe cria um DStream para os testes baseado em uma fila em memória. Você pode criar esta classe no pacote que for melhor para você.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;werneckpaiva.spark.test.util.TestInputDStream&lt;/strong&gt;&lt;/p&gt;


&lt;pre&gt;&lt;code class=&#34;language-python&#34; style=&#34;font-size: medium; line-height: 1.2;&#34;&gt;package werneckpaiva.spark.test.util

import scala.collection.mutable.ArrayBuffer
import scala.collection.mutable.Queue
import scala.reflect.ClassTag

import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.Time
import org.apache.spark.streaming.dstream.InputDStream


class TestInputDStream[T:ClassTag](
    ssc: StreamingContext, queue: Queue[RDD[T]], defaultRDD: RDD[T])
      extends InputDStream[T](ssc) {

  def start() {}
  def stop() {}
  def compute(validTime: Time): Option[RDD[T]] = {
    val buffer = new ArrayBuffer[RDD[T]]()
    if (!queue.isEmpty) {
      Some(queue.dequeue())
    } else {
      Some(defaultRDD)
    }
  }
}&lt;/code&gt;&lt;/pre&gt;


&lt;h3 id=&#34;classe-teste-base:616fff47681e64d3e324d29372e66883&#34;&gt;Classe Teste Base&lt;/h3&gt;

&lt;p&gt;A última classe a ser criada é uma classe utilitária para simplificar o código de nossos testes. Ela inicia o contexto Spark Streaming e declara um método que sincroniza nosso código até que os dados estejam prontos para que o streaming seja processado. Neste caso estamos sempre iniciando o streaming de teste com duração de 5 segundos. Você pode definir o período que achar melhor e colocar no pacote que achar apropriado.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;werneckpaiva.spark.test.BaseTestSparkStreaming&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34; style=&#34;font-size: medium; line-height: 1.2;&#34;&gt;package werneckpaiva.spark.test

import org.apache.spark.streaming.StreamingContext
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.junit.After
import org.junit.Before
import org.apache.spark.ClockWrapper
import org.apache.spark.streaming.Seconds
import java.nio.file.Files
import scala.reflect.ClassTag
import werneckpaiva.spark.test.util.TestInputDStream
import scala.collection.mutable.ListBuffer
import scala.collection.mutable.Queue
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.Duration

class BaseTestSparkStreaming {
  var ssc:StreamingContext = _
  var sc:SparkContext = _
  var clock:ClockWrapper = _

  @Before
  def before():Unit = {
    val sparkConf = new SparkConf()
      .setAppName(&amp;#34;Test Spark Streaming&amp;#34;)
      .setMaster(&amp;#34;local[*]&amp;#34;)
      .set(&amp;#34;spark.streaming.clock&amp;#34;, &amp;#34;org.apache.spark.streaming.util.ManualClock&amp;#34;)

    val checkpointDir = Files.createTempDirectory(&amp;#34;test&amp;#34;).toString
    ssc = new StreamingContext(sparkConf, Seconds(5))
    ssc.checkpoint(checkpointDir)
    sc = ssc.sparkContext
    clock = new ClockWrapper(ssc)
  }

  @After
  def after():Unit = {
    ssc.stop()
    sc.stop()
  }

  def makeStream[T:ClassTag]():(Queue[RDD[T]], TestInputDStream[T]) = {
    val lines = new Queue[RDD[T]]()
    val stream = new TestInputDStream[T](ssc, lines, sc.makeRDD(Seq[T](), 1))
    (lines, stream)
  }

  def waitForResultReady[T](stream:DStream[T], time:Duration):ListBuffer[Array[T]] = {
    val results = ListBuffer.empty[Array[T]]
    stream.foreachRDD((rdd, time) =&amp;gt; {
      results.append(rdd.collect())
    })
    ssc.start()
    clock.advance(time.milliseconds)
    for(i &amp;lt;- 1 to 100){
      if(results.length &amp;gt;= 1) return results
      Thread.sleep(100)
    }
    throw new Exception(&amp;#34;Can&amp;#39;t load stream&amp;#34;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;

&lt;h2 id=&#34;criando-seus-testes:616fff47681e64d3e324d29372e66883&#34;&gt;Criando seus testes&lt;/h2&gt;

&lt;p&gt;Depois de criar essas classes, já temos todas as ferramentas para construir nossos testes. Cada teste inicializa o DStream de teste e uma fila em memória. Executa o código a ser testado, e popula a fila com os dados de teste. Para podermos executar os asserts, precisamos antes executar o método que avança o relógio virtual.&lt;/p&gt;

&lt;p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34; style=&#34;font-size: medium; line-height: 1.2;&#34;&gt;class TestSparkStreaming extends BaseTestSparkStreaming{

  @Test
  def testSimpleSum():Unit = {
    val (lines, stream) = makeStream[(String, Long)]()

    // Código que você quer testar
    val reducedStream = StreamOperations.streamSum(stream)

    lines &amp;#43;= sc.makeRDD(Seq((&amp;#34;a&amp;#34;, 1L), (&amp;#34;a&amp;#34;, 1L), (&amp;#34;b&amp;#34;, 1L)))
    lines &amp;#43;= sc.makeRDD(Seq((&amp;#34;a&amp;#34;, 1L), (&amp;#34;a&amp;#34;, 1L)))
    lines &amp;#43;= sc.makeRDD(Seq((&amp;#34;a&amp;#34;, 1L), (&amp;#34;a&amp;#34;, 2L), (&amp;#34;b&amp;#34;, 3L), (&amp;#34;b&amp;#34;, 2L)))

    val results = waitForResultReady(reducedStream, Seconds(20))

    assertEquals((&amp;#34;a&amp;#34;, 2), results(0)(0))
    assertEquals((&amp;#34;b&amp;#34;, 1), results(0)(1))
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;

&lt;p&gt;No exemplo acima criamos um stream com 3 períodos representados por 3 sequência de dados. Nosso assert só está testando o primeiro período. O código testado não faz nada mais do que agrupar os valores por chave e somar os valores.&lt;/p&gt;

&lt;p&gt;Eu penei um pouco para chegar até essas classes e iniciar meus testes, mas agora é bem simples criá-los e isso aumentou bem a qualidade do código que produzo.&lt;/p&gt;

&lt;p&gt;O código desses exemplo está neste projeto do Github &lt;a href=&#34;https://github.com/werneckpaiva/spark-junit&#34;&gt;https://github.com/werneckpaiva/spark-junit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Espero ter ajudado.&lt;/p&gt;

&lt;p&gt;Post original em &lt;a href=&#34;http://blog.werneckpaiva.com.br/2016/03/testes-de-spark-streaming-com-junit/&#34;&gt;http://blog.werneckpaiva.com.br/2016/03/testes-de-spark-streaming-com-junit/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Testes de jobs Spark com JUnit</title>
      <link>http://grandesdados.com/post/testes-spark-junit/</link>
      <pubDate>Sat, 12 Mar 2016 01:30:34 -0300</pubDate>
      
      <guid>http://grandesdados.com/post/testes-spark-junit/</guid>
      <description>

&lt;p&gt;A boa prática de desenvolvimento de software diz que devemos criar sempre testes para nossos códigos, e no universo de Big Data não deveria ser diferente. Neste artigo apresento como testar um código Spark com JUnit para jobs que rodam em batch (não-streaming).&lt;/p&gt;

&lt;p&gt;Os códigos deste artigo estão disponíveis em &lt;a href=&#34;https://github.com/werneckpaiva/spark-junit&#34;&gt;https://github.com/werneckpaiva/spark-junit&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;build-e-gerenciamento-de-dependências:b5aa752cc1d5b8aaa758c40936f63c66&#34;&gt;Build e gerenciamento de dependências&lt;/h2&gt;

&lt;p&gt;O script abaixo define a configuração do build usando a ferramenta Gradle. Com ele baixamos as dependências do Spark para a nossa máquina e configuramos os testes&lt;/p&gt;


&lt;pre&gt;&lt;code class=&#34;language-javascript&#34; style=&#34;font-size: medium; line-height: 1.2;&#34;&gt;apply plugin: &amp;#39;scala&amp;#39;
sourceCompatibility = 1.7

repositories {
    mavenCentral()
}

dependencies {
    compile &amp;#39;org.slf4j:slf4j-api:1.7.12&amp;#39;
    compile &amp;#39;org.slf4j:slf4j-log4j12:1.7.12&amp;#39;
    compile &amp;#39;org.scala-lang:scala-library:2.10.6&amp;#39;
    compile(&amp;#39;org.apache.hadoop:hadoop-client:2.7.1&amp;#39;) {
        exclude group: &amp;#39;javax.servlet&amp;#39;
        exclude group: &amp;#39;javax.servlet.jsp&amp;#39;
        exclude group: &amp;#39;org.mortbay.jetty&amp;#39;
    }
    compile(&amp;#39;org.apache.spark:spark-core_2.10:1.6.1&amp;#39;) {
        exclude group: &amp;#39;org.apache.hadoop&amp;#39;
    }
    compile &amp;#39;org.apache.spark:spark-sql_2.10:1.6.1&amp;#39;
    compile &amp;#39;org.apache.spark:spark-streaming_2.10:1.6.1&amp;#39;
    testCompile &amp;#39;junit:junit:4.11&amp;#39;
}

test {
    testLogging {
        events &amp;#34;passed&amp;#34;, &amp;#34;skipped&amp;#34;, &amp;#34;failed&amp;#34;
        exceptionFormat &amp;#34;full&amp;#34;
    }
}&lt;/code&gt;&lt;/pre&gt;


&lt;h2 id=&#34;criando-a-classe-de-testes:b5aa752cc1d5b8aaa758c40936f63c66&#34;&gt;Criando a classe de testes&lt;/h2&gt;

&lt;p&gt;Ao criar um teste de um job Spark você precisa iniciar o SparkContext local antes de todos os testes e encerrá-lo no final. O ponto chave desta configuração é definir o master do Spark para &lt;code&gt;local&lt;/code&gt;, desta forma o Spark irá rodar localmente, sem depender de nenhum gerenciador de recursos (como o Yarn).&lt;/p&gt;

&lt;p&gt;Primeiro crie um object que inicia e termina o SparkContext no início dos testes:&lt;/p&gt;


&lt;pre&gt;&lt;code class=&#34;language-python&#34; style=&#34;font-size: medium; line-height: 1.2;&#34;&gt;object TestMyCode {
  var sc: SparkContext = _

  @BeforeClass
  def before(): Unit = {
    val sparkConf = new SparkConf()
      .setAppName(&amp;#34;Test Spark Batch&amp;#34;)
      .setMaster(&amp;#34;local&amp;#34;)
    sc = new SparkContext(sparkConf)
  }

  @AfterClass
  def after(): Unit = {
    sc.stop()
  }

}&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;Crie a classe com os seus testes. Inicie a classe permitindo que o SparkContext esteja disponível, e crie um contexto SQLContext baseado no SparkContext para poder fazer uso de recursos de DataFrame:&lt;/p&gt;

&lt;p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34; style=&#34;font-size: medium; line-height: 1.2;&#34;&gt;class TestMyCode {
  import TestMyCode._
  val sql = new SQLContext(sc)
  import sql.implicits._

  // ...
}&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;

&lt;p&gt;O código de seus testes devem começar criando um RDD utilizando o método &lt;code&gt;parallellize()&lt;/code&gt; do SparkContext. Se o seu código é baseado em DataFrames, você pode usar o método &lt;code&gt;toDF&lt;/code&gt;, disponível quando você importa o implict do SqlContext.&lt;/p&gt;

&lt;p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34; style=&#34;font-size: medium; line-height: 1.2;&#34;&gt;@Test
def testCountPairNumbers(): Unit = {
  val data = List(1, 2, 3, 4, 5, 6)
  val df =  sc.parallelize(data).toDF

  # Código que está querendo testar
  val count = MyOperations.countPairs(df)

  assertEquals(3, count)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;

&lt;h2 id=&#34;rodando-os-testes:b5aa752cc1d5b8aaa758c40936f63c66&#34;&gt;Rodando os testes&lt;/h2&gt;

&lt;p&gt;Para rodar os teste, você pode usar seu IDE (no Eclipse, clique com botão direito no arquivo do teste, selecione Run as &amp;gt; Scala JUnit Test). Podemos usar o Gradle para rodar todos os testes pela linha de comando:&lt;/p&gt;

&lt;p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34; style=&#34;font-size: medium; line-height: 1.2;&#34;&gt;gradle clean test&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;

&lt;p&gt;Pronto! Você já pode criar e rodar testes para seu código Spark. Espero que tenha ajudado. Em breve escreverei um post ensinando como testar um código de Spark Streaming.&lt;/p&gt;

&lt;p&gt;Post original em &lt;a href=&#34;http://blog.werneckpaiva.com.br/2016/03/testes-de-jobs-spark-com-junit/&#34;&gt;http://blog.werneckpaiva.com.br/2016/03/testes-de-jobs-spark-com-junit/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>