<!DOCTYPE html>
<html lang="pt-br">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

  	<meta property="og:title" content=" Configuração do Hadoop, HBase e Kafka na Máquina Local com Docker &middot;  Grandes Dados" />
  	<meta property="og:site_name" content="Grandes Dados" />
  	<meta property="og:url" content="http://grandesdados.com/post/configuracao-do-hadoop-hbase-e-kafka-na-maquina-local-com-docker/" />

    
        <meta property="og:image" content="http://grandesdados.com/images/placeholder-share.jpg" />
    


    
  	<meta property="og:type" content="article" />

    <meta property="og:article:published_time" content="2015-09-16T23:26:07-03:00" />

    
    <meta property="og:article:tag" content="Tutorial" />
    
    <meta property="og:article:tag" content="Hadoop" />
    
    <meta property="og:article:tag" content="HBase" />
    
    <meta property="og:article:tag" content="Kafka" />
    
    <meta property="og:article:tag" content="Docker" />
    
    

    <title>
         Configuração do Hadoop, HBase e Kafka na Máquina Local com Docker &middot;  Grandes Dados
    </title>

    
        <meta name="description" content="Blog de BigData" />
    

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="http://grandesdados.com/images/favicon.ico">
	  <link rel="apple-touch-icon" href="http://grandesdados.com/images/apple-touch-icon.png" />

    <link rel="stylesheet" type="text/css" href="http://grandesdados.com/css/screen.css" />
    <link rel="stylesheet" type="text/css" href="http://grandesdados.com/css/nav.css" />
    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Inconsolata" />


    
      
          <link href="http://grandesdados.com/index.xml" rel="alternate" type="application/rss+xml" title="Grandes Dados" />
      
      
    
    <meta name="generator" content="Hugo 0.15" />

    <link rel="canonical" href="http://grandesdados.com/post/configuracao-do-hadoop-hbase-e-kafka-na-maquina-local-com-docker/" />

    
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-38960920-6', 'auto');
      ga('send', 'pageview');

    </script>
    

    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.7/styles/solarized_dark.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.7/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

</head>
<body class="nav-closed">

  <div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
        
        
        
            
            <li class="nav-opened" role="presentation">
            	<a href="http://grandesdados.com/">Blog</a>
            </li>
        
            
            <li class="nav-opened" role="presentation">
            	<a href="https://github.com/GrandesDados">GitHub</a>
            </li>
        
    </ul>
    
    
    <a class="subscribe-button icon-feed" href="http://grandesdados.com/index.xml">Subscribe</a>
    
</div>
<span class="nav-cover"></span>


 <div class="site-wrapper">




<header class="main-header post-head no-cover">
  <nav class="main-nav clearfix">


  
  
      <a class="menu-button" href="#"><span class="burger">&#9776;</span><span class="word">Menu</span></a>
  
  </nav>
</header>



<main class="content" role="main">




  <article class="post post">

    <header class="post-header">
        <h1 class="post-title">Configuração do Hadoop, HBase e Kafka na Máquina Local com Docker</h1>
        <small></small>

        <section class="post-meta">
        
          <time class="post-date" datetime="2015-09-16T23:26:07-03:00">
            Sep 16, 2015
          </time>
        
         
          <span class="post-tag small"><a href="http://grandesdados.com/tags/tutorial/">#Tutorial</a></span>
         
          <span class="post-tag small"><a href="http://grandesdados.com/tags/hadoop/">#Hadoop</a></span>
         
          <span class="post-tag small"><a href="http://grandesdados.com/tags/hbase/">#HBase</a></span>
         
          <span class="post-tag small"><a href="http://grandesdados.com/tags/kafka/">#Kafka</a></span>
         
          <span class="post-tag small"><a href="http://grandesdados.com/tags/docker/">#Docker</a></span>
         
        </section>
    </header>
  
    <section class="post-content">
      

<p>Esse tutorial é sobre a criação de uma imagem do Docker com a configuração local do Hadoop, HBase e Kafka. Nesse procedimento, o Hadoop é configurado no modo pseudo-distribuído com cada serviço rodando em uma instância própria da JVM, mas todas na mesma máquina. O HBase e o Kafka também rodam em modo &lsquo;distribuído&rsquo; compartilhando uma instância separada do ZooKeeper. Esse procedimento é muito útil para testar funcionalidades desses serviços e aprendizado, mas não é uma solução completa para uso em produção.</p>

<h2 id="pré-requisito:7662b18c2fa1b345b82a7d885cf16b10">Pré-requisito</h2>

<p>Nesse procedimento, é necessário que o Docker esteja instalado e funcionando; também é necessário acesso à Internet.</p>

<p>Originalmente, esse procedimento foi testado no ArchLinux atualizado até final de Agosto/2015.</p>

<p><a href="https://wiki.archlinux.org/index.php/Docker">https://wiki.archlinux.org/index.php/Docker</a></p>


<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">sudo docker version

&gt; Client:
&gt;  Version:      1.8.1
&gt;  API version:  1.20
&gt;  Go version:   go1.4.2
&gt;  Git commit:   d12ea79
&gt;  Built:        Sat Aug 15 17:29:10 UTC 2015
&gt;  OS/Arch:      linux/amd64
&gt;
&gt; Server:
&gt;  Version:      1.8.1
&gt;  API version:  1.20
&gt;  Go version:   go1.4.2
&gt;  Git commit:   d12ea79
&gt;  Built:        Sat Aug 15 17:29:10 UTC 2015
&gt;  OS/Arch:      linux/amd64</code></pre>


<h2 id="configuração:7662b18c2fa1b345b82a7d885cf16b10">Configuração</h2>

<p>Hadoop, ZooKeeper, HBase e Kafka.</p>

<h3 id="container:7662b18c2fa1b345b82a7d885cf16b10">Container</h3>

<p>Começamos com a criação de um conainer do Docker com a imagem do CentOS6.</p>

<blockquote>
<p>Importante: para os endereços com <code>grandesdados-hadoop</code> funcionarem fora do container, direto na máquina host, é necessário colocar no <code>/etc/hosts</code> da máquina host o endereço IP do container do Docker para esse nome.</p>
</blockquote>

<p>Ao executar o comando <code>run</code>, o Docker automaticamente fará o download da imagem e a shell será inicializada dentro de um novo container.</p>


<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">sudo docker run -i -t --name=grandesdados-hadoop --hostname=grandesdados-hadoop centos:6 /bin/bash

&gt; Unable to find image &#39;centos:6&#39; locally
&gt; 6: Pulling from library/centos
&gt;
&gt; f1b10cd84249: Pull complete
&gt; fb9cc58bde0c: Pull complete
&gt; a005304e4e74: Already exists
&gt; library/centos:6: The image you are pulling has been verified. Important: image verification is a tech preview feature and should not be relied on to provide security.
&gt;
&gt; Digest: sha256:25d94c55b37cb7a33ad706d5f440e36376fec20f59e57d16fe02c64698b531c1
&gt; Status: Downloaded newer image for centos:6
&gt; [root@grandesdados-hadoop /]#</code></pre>


<p>Já dentro do container criamos um usuário e local que serão usados para a instalação e execução dos processos.</p>


<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">adduser -m -d /hadoop hadoop
cd hadoop</code></pre>


<p>A versão usada nesse procedimento é o Java 8, atual versão estável da Oracle.</p>


<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">curl -k -L -H &#34;Cookie: oraclelicense=accept-securebackup-cookie&#34; -O http://download.oracle.com/otn-pub/java/jdk/8u60-b27/jdk-8u60-linux-x64.rpm
rpm -i jdk-8u60-linux-x64.rpm

echo &#39;export JAVA_HOME=&#34;/usr/java/jdk1.8.0_60&#34;&#39; &gt; /etc/profile.d/java.sh
source /etc/profile.d/java.sh

echo $JAVA_HOME

&gt; /usr/java/jdk1.8.0_60

java -version

&gt; java version &#34;1.8.0_60&#34;
&gt; Java(TM) SE Runtime Environment (build 1.8.0_60-b27)
&gt; Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)</code></pre>


<p>Para completar o ambiente de execução, instalamos os serviços e bibliotecas necessárias.</p>


<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">yum install -y tar openssh-clients openssh-server rsync gzip zlib openssl fuse bzip2 snappy

&gt; (...)</code></pre>


<p>(configuração do SSH para acesso sem senha)</p>


<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">service sshd start
chkconfig sshd on

su - hadoop

ssh-keygen -C hadoop -P &#39;&#39; -f ~/.ssh/id_rsa
cp ~/.ssh/{id_rsa.pub,authorized_keys}

ssh-keyscan grandesdados-hadoop &gt;&gt;  ~/.ssh/known_hosts
ssh-keyscan localhost &gt;&gt; ~/.ssh/known_hosts
ssh-keyscan 127.0.0.1 &gt;&gt; ~/.ssh/known_hosts
ssh-keyscan 0.0.0.0 &gt;&gt; ~/.ssh/known_hosts

ssh grandesdados-hadoop

&gt; Warning: Permanently added the RSA host key for IP address &#39;172.17.0.12&#39; to the list of known hosts.
&gt; (nova shell, sem login nem confirmação)

# (sair do shell do ssh)
exit
# (sair do shell do su)
exit

whoami

&gt; root</code></pre>


<h3 id="hadoop:7662b18c2fa1b345b82a7d885cf16b10">Hadoop</h3>

<p>Procedimento para configuração local do Hadoop em modo pseudo-distribuído com uma JVM por serviço.</p>

<p>Esse procedimento é baseado na <a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/SingleCluster.html">documentação do Hadoop</a>.</p>

<p>Serviços:</p>

<ul>
<li>HDFS: NameNode, SecondaryNameNode, DataNode</li>
<li>YARN: ResouceManager, NodeManager</li>
<li>MR: HistoryServer</li>
</ul>

<p>&hellip;</p>

<p><strong>Instalação</strong></p>

<p>O pacote usado nesse procedimento é o Hadoop 2.7.1 para CentOS6 descrito outro <a href="http://grandesdados.com/post/compilacao-do-hadoop-para-centos6-rhel6-usando-docker/">artigo</a>.</p>

<p>Primeiramente, colocamos o pacote dentro do container.</p>


<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;"># (shell fora do container)
sudo docker cp hadoop-2.7.1.tar.gz grandesdados-hadoop:/hadoop</code></pre>


<p>De volta ao container como usuário root.</p>


<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">tar zxf hadoop-2.7.1.tar.gz -C /opt
chown hadoop:hadoop -R /opt/hadoop-2.7.1

echo &#39;export PATH=$PATH:/opt/hadoop-2.7.1/bin:/opt/hadoop-2.7.1/sbin&#39; &gt; /etc/profile.d/hadoop.sh
source /etc/profile.d/hadoop.sh

hadoop version

&gt; Hadoop 2.7.1
&gt; Subversion Unknown -r Unknown
&gt; Compiled by hadoop on 2015-09-01T00:30Z
&gt; Compiled with protoc 2.5.0
&gt; From source with checksum fc0a1a23fc1868e4d5ee7fa2b28a58a
&gt; This command was run using /opt/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar

mkdir -p /data/hadoop
chown hadoop:hadoop /data/hadoop</code></pre>


<p><strong>Configuração</strong></p>

<p>(para a configuração, deve ser usado o usuário hadoop: <code>su - hadoop</code>)</p>

<p>Editar <code>/opt/hadoop-2.7.1/etc/hadoop/core-site.xml</code>:</p>


<pre><code class="language-xml" style="font-size: medium; line-height: 1.2;">&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;/data/hadoop&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://grandesdados-hadoop&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre>


<p>Editar <code>/opt/hadoop-2.7.1/etc/hadoop/hdfs-site.xml</code>:</p>


<pre><code class="language-xml" style="font-size: medium; line-height: 1.2;">&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.blocksize&lt;/name&gt;
        &lt;value&gt;8M&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre>


<p>Editar <code>/opt/hadoop-2.7.1/etc/hadoop/yarn-site.xml</code>:</p>


<pre><code class="language-xml" style="font-size: medium; line-height: 1.2;">&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre>


<p>Editar <code>/opt/hadoop-2.7.1/etc/hadoop/mapred-site.xml</code>:</p>


<pre><code class="language-xml" style="font-size: medium; line-height: 1.2;">&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.jobtracker.staging.root.dir&lt;/name&gt;
        &lt;value&gt;/user&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre>


<p>Setup Inicial (antes da primeira inicialização).</p>


<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">hdfs namenode -format

&gt; 15/09/16 02:12:03 INFO namenode.NameNode: STARTUP_MSG: 
&gt; /************************************************************
&gt; STARTUP_MSG: Starting NameNode
&gt; STARTUP_MSG:   host = grandesdados-hadoop/172.17.0.12
&gt; STARTUP_MSG:   args = [-format]
&gt; STARTUP_MSG:   version = 2.7.1
&gt; (...)
&gt; INFO namenode.NameNode: createNameNode [-format]
&gt; Formatting using clusterid: CID-5daa32a0-3ab6-405e-bfd2-05c0a6e1e7e6
&gt; (...)
&gt; INFO common.Storage: Storage directory /data/hadoop/dfs/name has been successfully formatted.
&gt; (...)</code></pre>


<p><strong>HDFS</strong></p>

<p>(como usuário hadoop <code>su - hadoop</code>)</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">start-dfs.sh

&gt; Starting namenodes on [grandesdados-hadoop]
&gt; grandesdados-hadoop: starting namenode, logging to /opt/hadoop-2.7.1/logs/hadoop-hadoop-namenode-grandesdados-hadoop.out
&gt; localhost: starting datanode, logging to /opt/hadoop-2.7.1/logs/hadoop-hadoop-datanode-grandesdados-hadoop.out
&gt; Starting secondary namenodes [0.0.0.0]
&gt; 0.0.0.0: starting secondarynamenode, logging to /opt/hadoop-2.7.1/logs/hadoop-hadoop-secondarynamenode-grandesdados-hadoop.out

# criação do diretório do usuário hadoop
hdfs dfs -mkdir -p /user/hadoop</code></pre>
</p>

<p>Interface Web do Name Node:</p>

<p><a href="http://grandesdados-hadoop:50070/">http://grandesdados-hadoop:50070/</a></p>

<p>Interface Web do Data Node (vazia):</p>

<p><a href="http://grandesdados-hadoop:50075/">http://grandesdados-hadoop:50075/</a></p>

<p>Interface Web do Secondary Name Node:</p>

<p><a href="http://grandesdados-hadoop:50090/">http://grandesdados-hadoop:50090/</a></p>

<p>Para parar o serviço:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">stop-dfs.sh</code></pre>
</p>

<p><strong>YARN</strong></p>

<p>(como usuário hadoop <code>su - hadoop</code>)</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">start-yarn.sh

&gt; starting yarn daemons
&gt; starting resourcemanager, logging to /opt/hadoop-2.7.1/logs/yarn-hadoop-resourcemanager-grandesdados-hadoop.out
&gt; localhost: starting nodemanager, logging to /opt/hadoop-2.7.1/logs/yarn-hadoop-nodemanager-grandesdados-hadoop.out</code></pre>
</p>

<p>Interface Web do Resource Manager:</p>

<p><a href="http://grandesdados-hadoop:8088/">http://grandesdados-hadoop:8088/</a></p>

<p>Interface Web do Node Manager:</p>

<p><a href="http://grandesdados-hadoop:8042/">http://grandesdados-hadoop:8042/</a></p>

<p>Para parar o serviço:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">stop-yarn.sh</code></pre>
</p>

<p><strong>History Server</strong></p>

<p>(como usuário hadoop <code>su - hadoop</code>)</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">mr-jobhistory-daemon.sh start historyserver

&gt; starting historyserver, logging to /opt/hadoop-2.7.1/logs/mapred-hadoop-historyserver-grandesdados-hadoop.out</code></pre>
</p>

<p>Interface Web do History Server:</p>

<p><a href="http://grandesdados-hadoop:19888/">http://grandesdados-hadoop:19888/</a></p>

<p>Para parar o serviço:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">mr-jobhistory-daemon.sh stop historyserver</code></pre>
</p>

<p><strong>Teste</strong></p>

<p>(para os testes, deve ser usado o usuário hadoop: <code>su - hadoop</code>)</p>

<p>Processos:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">ps x

&gt;   PID TTY      STAT   TIME COMMAND
&gt;  5162 ?        S      0:00 -bash
&gt;  5327 ?        Sl     0:08 /usr/java/jdk1.8.0_60/bin/java -Dproc_namenode (...)
&gt;  5423 ?        Sl     0:07 /usr/java/jdk1.8.0_60/bin/java -Dproc_datanode (...)
&gt;  5612 ?        Sl     0:06 /usr/java/jdk1.8.0_60/bin/java -Dproc_secondarynamenode (...)
&gt;  5772 ?        Sl     0:08 /usr/java/jdk1.8.0_60/bin/java -Dproc_resourcemanager (...)
&gt;  5870 ?        Sl     0:07 /usr/java/jdk1.8.0_60/bin/java -Dproc_nodemanager (...)
&gt;  6189 ?        Sl     0:08 /usr/java/jdk1.8.0_60/bin/java -Dproc_historyserver (...)
&gt;  6273 ?        R&#43;     0:00 ps x</code></pre>
</p>

<p>Exemplos de MapReduce:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">yarn jar /opt/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar

&gt; An example program must be given as the first argument.
&gt; Valid program names are:
&gt;   aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.
&gt;   aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.
&gt;   bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.
&gt;   dbcount: An example job that count the pageview counts from a database.
&gt;   distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.
&gt;   grep: A map/reduce program that counts the matches of a regex in the input.
&gt;   join: A job that effects a join over sorted, equally partitioned datasets
&gt;   multifilewc: A job that counts words from several files.
&gt;   pentomino: A map/reduce tile laying program to find solutions to pentomino problems.
&gt;   pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.
&gt;   randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.
&gt;   randomwriter: A map/reduce program that writes 10GB of random data per node.
&gt;   secondarysort: An example defining a secondary sort to the reduce.
&gt;   sort: A map/reduce program that sorts the data written by the random writer.
&gt;   sudoku: A sudoku solver.
&gt;   teragen: Generate data for the terasort
&gt;   terasort: Run the terasort
&gt;   teravalidate: Checking results of terasort
&gt;   wordcount: A map/reduce program that counts the words in the input files.
&gt;   wordmean: A map/reduce program that counts the average length of the words in the input files.
&gt;   wordmedian: A map/reduce program that counts the median length of the words in the input files.
&gt;   wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.</code></pre>
</p>

<p>Rodando o Cálculo do Pi com MapReduce:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">yarn jar /opt/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar pi 16 100000

&gt; Number of Maps  = 16
&gt; Samples per Map = 100000
&gt; (...)
&gt; INFO impl.YarnClientImpl: Submitted application application_1442439610364_0001
&gt; INFO mapreduce.Job: The url to track the job: http://grandesdados-hadoop:8088/proxy/application_1442439610364_0001/
&gt; INFO mapreduce.Job: Running job: job_1442439610364_0001
&gt; (...)
&gt; Job Finished in 48.333 seconds
&gt; Estimated value of Pi is 3.14157500000000000000</code></pre>
</p>

<h3 id="zookeeper:7662b18c2fa1b345b82a7d885cf16b10">ZooKeeper</h3>

<p>Esse procedimento é baseado na <a href="https://zookeeper.apache.org/doc/r3.4.6/zookeeperStarted.html">documentação do ZooKeeper</a>.</p>

<p>Dentro do container como usuário root:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">curl -L -O http://archive.apache.org/dist/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz
tar zxf zookeeper-3.4.6.tar.gz -C /opt
chown hadoop:hadoop -R /opt/zookeeper-3.4.6

echo &#39;export PATH=$PATH:/opt/zookeeper-3.4.6/bin&#39; &gt; /etc/profile.d/zookeeper.sh
source /etc/profile.d/zookeeper.sh

mkdir -p /data/zookeeper
chown hadoop:hadoop /data/zookeeper</code></pre>
</p>

<p>Editar <code>/opt/zookeeper-3.4.6/conf/zoo.cfg</code>:</p>

<p>
<pre><code class="language-ini" style="font-size: medium; line-height: 1.2;">tickTime=6000
dataDir=/data/zookeeper
clientPort=2181</code></pre>
</p>

<p>Inicializando o serviço:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">su - hadoop
zkServer.sh start

&gt; JMX enabled by default
&gt; Using config: /opt/zookeeper-3.4.6/bin/../conf/zoo.cfg
&gt; Starting zookeeper ... STARTED</code></pre>
</p>

<p>Para parar o serviço:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">zkServer.sh stop</code></pre>
</p>

<p><strong>Teste</strong></p>

<p>(para os testes, deve ser usado o usuário hadoop: <code>su - hadoop</code>)</p>

<p>Processo:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">ps x | grep zoo

&gt; 8246 ?        Sl     0:00 /usr/java/jdk1.8.0_60/bin/java (...) org.apache.zookeeper.server.quorum.QuorumPeerMain (...)
&gt; 8291 ?        S&#43;     0:00 grep zoo</code></pre>
</p>

<p>Telnet:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">echo &#39;ruok&#39; |  curl telnet://grandesdados-hadoop:2181

&gt; imok</code></pre>
</p>

<p>Cliente:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">zkCli.sh -server grandesdados-hadoop:2181

&gt; Connecting to grandesdados-hadoop:2181
&gt; ...
&gt; [zk: grandesdados-hadoop:2181(CONNECTED) 0] ls /
&gt; [zookeeper]
&gt; [zk: grandesdados-hadoop:2181(CONNECTED) 1] help
&gt; ZooKeeper -server host:port cmd args
&gt; 	stat path [watch]
&gt; 	set path data [version]
&gt; 	ls path [watch]
&gt; 	delquota [-n|-b] path
&gt; 	ls2 path [watch]
&gt; 	setAcl path acl
&gt; 	setquota -n|-b val path
&gt; 	history 
&gt; 	redo cmdno
&gt; 	printwatches on|off
&gt; 	delete path [version]
&gt; 	sync path
&gt; 	listquota path
&gt; 	rmr path
&gt; 	get path [watch]
&gt; 	create [-s] [-e] path data acl
&gt; 	addauth scheme auth
&gt; 	quit 
&gt; 	getAcl path
&gt; 	close 
&gt; 	connect host:port
&gt; [zk: grandesdados-hadoop:2181(CONNECTED) 3] quit</code></pre>
</p>

<h3 id="hbase:7662b18c2fa1b345b82a7d885cf16b10">HBase</h3>

<p>Esse procedimento é baseado na <a href="http://hbase.apache.org/book.html#quickstart">documentação do HBase</a>.</p>

<p>Dentro do container como usuário root:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">curl -L -O http://archive.apache.org/dist/hbase/1.1.2/hbase-1.1.2-bin.tar.gz
tar zxf hbase-1.1.2-bin.tar.gz -C /opt
chown hadoop:hadoop -R /opt/hbase-1.1.2

echo &#39;export PATH=$PATH:/opt/hbase-1.1.2/bin&#39; &gt; /etc/profile.d/hbase.sh
source /etc/profile.d/hbase.sh

mkdir -p /data/hbase/tmp
chown hadoop:hadoop -R /data/hbase</code></pre>
</p>

<p>Editar <code>/opt/hbase-1.1.2/conf/hbase-site.xml</code>:</p>

<p>
<pre><code class="language-xml" style="font-size: medium; line-height: 1.2;">&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;hdfs:///hbase&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.tmp.dir&lt;/name&gt;
    &lt;value&gt;/data/hbase/tmp&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
    &lt;value&gt;grandesdados-hadoop&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</code></pre>
</p>

<p>Editar <code>/opt/hbase-1.1.2/conf/hbase-env.sh</code>:
<br/>(manter conteúdo original, só alterar os valores abaixo)</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">export HBASE_OPTS=&#34;-XX:&#43;UseConcMarkSweepGC -Djava.net.preferIPv4Stack=true&#34;
export HBASE_MANAGES_ZK=false</code></pre>
</p>

<p>Inicializando o serviço:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">su - hadoop
start-hbase.sh

&gt; starting master, logging to /opt/hbase-1.1.2/bin/../logs/hbase-hadoop-master-grandesdados-hadoop.out
&gt; Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0
&gt; Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0
&gt; starting regionserver, logging to /opt/hbase-1.1.2/bin/../logs/hbase-hadoop-1-regionserver-grandesdados-hadoop.out</code></pre>
</p>

<p>Interface Web do Master:</p>

<p><a href="http://grandesdados-hadoop:16010/">http://grandesdados-hadoop:16010/</a></p>

<p>Interface Web do Region Server:</p>

<p><a href="http://grandesdados-hadoop:16301/">http://grandesdados-hadoop:16301/</a></p>

<p>Para parar o serviço:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">stop-hbase.sh</code></pre>
</p>

<p><strong>Teste</strong></p>

<p>(para os testes, deve ser usado o usuário hadoop: <code>su - hadoop</code>)</p>

<p>Processo:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">ps x | grep hbase

&gt; 8790 ?        S      0:00 bash /opt/hbase-1.1.2/bin/hbase-daemon.sh --config /opt/hbase-1.1.2/bin/../conf foreground_start master
&gt; 8804 ?        Sl     0:14 /usr/java/jdk1.8.0_60/bin/java -Dproc_master (...)
&gt; 8915 ?        S      0:00 bash /opt/hbase-1.1.2/bin/hbase-daemon.sh --config /opt/hbase-1.1.2/bin/../conf foreground_start regionserver -D hbase.regionserver.port=16201 -D hbase.regionserver.info.port=16301
&gt; 8929 ?        Sl     0:14 /usr/java/jdk1.8.0_60/bin/java -Dproc_regionserver (...)
&gt; 9329 ?        S&#43;     0:00 grep hbase</code></pre>
</p>

<p>Cliente:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">hbase shell

&gt; HBase Shell; enter &#39;help&lt;RETURN&gt;&#39; for list of supported commands.
&gt; Type &#34;exit&lt;RETURN&gt;&#34; to leave the HBase Shell
&gt; Version 1.1.2, rcc2b70cf03e3378800661ec5cab11eb43fafe0fc, Wed Aug 26 20:11:27 PDT 2015
&gt;
&gt; hbase(main):001:0&gt; status
&gt; 1 servers, 0 dead, 2.0000 average load
&gt;
&gt; hbase(main):002:0&gt; help
&gt; HBase Shell, version 1.1.2, rcc2b70cf03e3378800661ec5cab11eb43fafe0fc, Wed Aug 26 20:11:27 PDT 2015
&gt; Type &#39;help &#34;COMMAND&#34;&#39;, (e.g. &#39;help &#34;get&#34;&#39; -- the quotes are necessary) for help on a specific command.
&gt; Commands are grouped. Type &#39;help &#34;COMMAND_GROUP&#34;&#39;, (e.g. &#39;help &#34;general&#34;&#39;) for help on a command group.
&gt; (...)
&gt; hbase(main):004:0&gt; exit</code></pre>
</p>

<h3 id="kafka:7662b18c2fa1b345b82a7d885cf16b10">Kafka</h3>

<p>Esse procedimento é baseado na <a href="http://kafka.apache.org/documentation.html#quickstart">documentação do Kafka</a>.</p>

<p>Dentro do container como usuário root:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">curl -L -O http://archive.apache.org/dist/kafka/0.8.2.1/kafka_2.10-0.8.2.1.tgz
tar zxf kafka_2.10-0.8.2.1.tgz -C /opt
chown hadoop:hadoop -R /opt/kafka_2.10-0.8.2.1

echo &#39;export PATH=$PATH:/opt/kafka_2.10-0.8.2.1/bin&#39; &gt; /etc/profile.d/kafka.sh
source /etc/profile.d/kafka.sh

mkdir -p /data/kafka
chown hadoop:hadoop /data/kafka</code></pre>
</p>

<p>Editar <code>/opt/kafka_2.10-0.8.2.1/config/server.properties</code>:
<br/>(manter conteúdo original, só alterar os valores abaixo)</p>

<p>
<pre><code class="language-ini" style="font-size: medium; line-height: 1.2;">log.dirs=/data/kafka
zookeeper.connect=grandesdados-hadoop:2181</code></pre>
</p>

<p>Inicializando o serviço:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">su - hadoop
kafka-server-start.sh /opt/kafka_2.10-0.8.2.1/config/server.properties &amp;&gt; kafka.out &amp;</code></pre>
</p>

<p>Para parar o serviço:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">kafka-server-stop.sh /opt/kafka_2.10-0.8.2.1/config/server.properties</code></pre>
</p>

<p><strong>Teste</strong></p>

<p>(para os testes, deve ser usado o usuário hadoop: <code>su - hadoop</code>)</p>

<p>Processo:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">ps x | grep kafka

&gt; 9818 ?        Sl     0:03 /usr/java/jdk1.8.0_60/bin/java (...) kafka.Kafka /opt/kafka_2.10-0.8.2.1/config/server.properties
&gt; 9928 ?        S&#43;     0:00 grep kafka</code></pre>
</p>

<p>Enviando e recebendo mensagens:</p>

<p>
<pre><code class="language-sh" style="font-size: medium; line-height: 1.2;">kafka-topics.sh \
--create \
--zookeeper grandesdados-hadoop:2181 \
--replication-factor 1 \
--partitions 1 \
--topic test

&gt; Created topic &#34;test&#34;.

echo &#39;Primeira mensagem de teste&#39; | kafka-console-producer.sh --broker-list grandesdados-hadoop:9092 --topic test

&gt; (...)

kafka-console-consumer.sh --zookeeper grandesdados-hadoop:2181 --topic test --from-beginning

&gt; Primeira mensagem de teste
&gt; ^CConsumed 1 messages</code></pre>
</p>

<h2 id="conclusão:7662b18c2fa1b345b82a7d885cf16b10">Conclusão</h2>

<p>A revolução em BigData é um fenômeno da tecnologia desenvolvida ao longo dos últimos anos focada na manipulação de um grande volume de dados em máquinas de baixo custo. Essa é a tecnologia que torna possível combinar uma solução de dados escalável com processos para geração de resultados relevantes, tanto no desenvolvimento de produtos quanto na evolução do conhecimento. O importante é entender como essa tecnologia pode ser usada para agregar valor ao negócio e permitir imaginar soluções inovadoras.</p>

<p>Esse artigo documenta o passo-a-passo de uma configuração local do Hadoop, ZooKeeper, HBase e Kafka. Esses são os serviços essenciais em uma plataforma de BigData que, juntamente com o Spark, possibilitam o desenvolvimento de soluções tanto para processamento batch quanto para tempo real.</p>

<p>Em artigos futuros, entrarei usando essa solução para desenvolver e testar algumas aplicações de BigData usando Spark.</p>

<p>&hellip;</p>

<p>Você trabalha com Hadoop, HBase, Kafka ou tem experiência com as tecnologias envolvidas? Venha trabalhar conosco.</p>

<p><a href="http://talentos.globo.com/">http://talentos.globo.com/</a></p>

    </section>


  <footer class="post-footer">


    

    





<section class="author">
  <h4><a href="http://grandesdados.com/">Ciro Cavani</a></h4>
  
  <p>Sou formado em Engenharia de Computação pelo Instituto Tecnológico de Aeronáutica (ITA) e faço mestrado em Inteligência Artificial na Pontifícia Universidade Católica do Rio de Janeiro (PUC-Rio). Trabalho na Globo.com no time de Personalização que é responsável pelo desenvolvimento da Plataforma de BigData usada em Produtos de Dados, como o Sistema de Recomendação. Tenho experiência no desenvolvimento de plataformas para coleta, armazenamento e análise de grande volume de dados com objetivo de agregar valor ao negócio da empresa.</p>
  
  <div class="author-meta">
    <span class="author-location icon-location">Rio de Janeiro, RJ</span>
    <span class="author-link icon-link"><a href="https://www.linkedin.com/in/cirocavani">https://www.linkedin.com/in/cirocavani</a></span>
  </div>
</section>



    
    <section class="share">
      <h4>Share this post</h4>
      <a class="icon-twitter" style="font-size: 1.4em" href="https://twitter.com/share?text=Configura%c3%a7%c3%a3o%20do%20Hadoop%2c%20HBase%20e%20Kafka%20na%20M%c3%a1quina%20Local%20com%20Docker&amp;url=http%3a%2f%2fgrandesdados.com%2fpost%2fconfiguracao-do-hadoop-hbase-e-kafka-na-maquina-local-com-docker%2f"
          onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
          <span class="hidden">Twitter</span>
      </a>
      <a class="icon-facebook" style="font-size: 1.4em" href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2fgrandesdados.com%2fpost%2fconfiguracao-do-hadoop-hbase-e-kafka-na-maquina-local-com-docker%2f"
          onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
          <span class="hidden">Facebook</span>
      </a>
      <a class="icon-google-plus" style="font-size: 1.4em" href="https://plus.google.com/share?url=http%3a%2f%2fgrandesdados.com%2fpost%2fconfiguracao-do-hadoop-hbase-e-kafka-na-maquina-local-com-docker%2f"
         onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
          <span class="hidden">Google+</span>
      </a>
    </section>
    

    
    
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname = 'grandesdados';
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    
    

  </footer>
</article>

</main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="">Grandes Dados</a> Todos os direitos reservados - 2015</section>
        
        <section class="poweredby">Proudly generated by <a class="icon-hugo" href="http://gohugo.io">HUGO</a>, with <a class="icon-theme" href="https://github.com/vjeantet/hugo-theme-casper">Casper</a> theme</section>
        
    </footer>
    </div>
    <script type="text/javascript" src="http://grandesdados.com/js/jquery.js"></script>
    <script type="text/javascript" src="http://grandesdados.com/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="http://grandesdados.com/js/index.js"></script>

</body>
</html>

